{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "import dalex as dx\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('BankChurners.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need the unique ids'\n",
    "full_df.drop('CLIENTNUM', axis=1, inplace=True)\n",
    "\n",
    "full_df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'], \n",
    "          inplace=True, axis=1)\n",
    "full_df.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], \n",
    "          inplace=True, axis=1)\n",
    "\n",
    "display(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.DataFrame()\n",
    "\n",
    "def tobinary():\n",
    "    \n",
    "    # full_df['Attrition_Flag'] = full_df.Attrition_Flag // same thing\n",
    "    updated_df['Attrition'] = full_df.Attrition_Flag.map({'Existing Customer':1, 'Attrited Customer':0})\n",
    "    \n",
    "    updated_df['Gender'] = full_df.Gender.map({'M':1, 'F':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['Customer_Age','Credit_Limit','Months_on_book','Avg_Utilization_Ratio','Total_Trans_Amt','Dependent_count',\n",
    "                  'Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon','Total_Revolving_Bal',\n",
    "                  'Total_Amt_Chng_Q4_Q1','Total_Ct_Chng_Q4_Q1']\n",
    "\n",
    "def stringtoint():\n",
    "    missing_income = full_df['Income_Category'].replace({'Unknown': 1 , 'Less than $40K':0, '$40K - $60K':0, \n",
    "                                                      '$80K - $120K':0, '$60K - $80K':0, '$120K +':0})\n",
    "    #missinng data will be replaced with mode:\n",
    "    income_data    = full_df['Income_Category'].replace({'Unknown': 1 , 'Less than $40K':1, '$40K - $60K':2, \n",
    "                                                      '$80K - $120K':4, '$60K - $80K':3, '$120K +':5})\n",
    "    \n",
    "    \n",
    "    \n",
    "    missing_education = full_df['Education_Level'].replace({'Unknown': 1, 'High School':0, 'Graduate':0, 'Uneducated':0,\n",
    "                                                         'College':0,'Post-Graduate':0,'Doctorate':0})\n",
    "    #missinng data will be replaced with mode:\n",
    "    education_data    = full_df['Education_Level'].replace({'Unknown': 4, 'High School':2, 'Graduate':4, 'Uneducated':1,\n",
    "                                                         'College':3,'Post-Graduate':5,'Doctorate':6})\n",
    "    \n",
    "    updated_df['Missing_Income']    = missing_income\n",
    "    updated_df['Income_Category']   = income_data\n",
    "    updated_df['Missing_Education'] = missing_education\n",
    "    updated_df['Education_Level']   = education_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode():\n",
    "    global updated_df\n",
    "    card_dummies = pd.get_dummies(full_df['Card_Category'], prefix='Card')\n",
    "    marital_dummies = pd.get_dummies(full_df['Marital_Status'], prefix='Marital')\n",
    "    updated_df = pd.concat([updated_df, marital_dummies, card_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_with_numerics():\n",
    "    global updated_df\n",
    "    updated_df = pd.concat([updated_df, full_df.loc[:, numeric_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobinary()\n",
    "stringtoint()\n",
    "encode()\n",
    "concat_with_numerics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_logreg = LogisticRegression()\n",
    "\n",
    "# m_logreg = pickle.load(open(\"m_logreg\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(updated_df.drop('Attrition', axis = 1), updated_df.Attrition, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_logreg = m_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = m_logreg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred, squared = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.plot_roc_curve(m_logreg, X_test, y_test)  \n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-answer",
   "metadata": {},
   "source": [
    "# Dla wybranej obserwacji ze zbioru danych wylicz predykcję modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dx.Explainer(m_logreg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.predict(X_train)[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-bhutan",
   "metadata": {},
   "source": [
    "Do zadania został użyty model regresji logicznej. Przglądając różne obserwację otrzymywałam takie wyniki jak: 0,34; 0,76; 0,98. My przyjrzymy się obserwacji o indexie 4, której predykcja wynosi ok.0.84."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-scope",
   "metadata": {},
   "source": [
    "# Dla wybranej obserwacji z punktu 1., wylicz dekompozycję predykcji modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = explainer.predict_parts(X_train.iloc[4,:], type='shap')\n",
    "pp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-water",
   "metadata": {},
   "source": [
    "Dla obserwacji o indexie 4 największą udział w predykcji mają zmienne:\n",
    "- `Total_Relationship_Count` = 6, klient korzysta intensywnie z usług, więc nie ma zamiaru zamykać konta\n",
    "- `Total_Revolving_Bal` = 0, klient ma bardzo ograniczone możliwości korzystania z karty, więc ujemny udział zmiennej jest może być uzasadniony\n",
    "- `Contacts_Count_12_mon` = 4, klient nie zrezygnuje z usługi, ponieważ potrzebuję konta do prowadzenia tranzakcji ze swoimi \"kontaktami\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-people",
   "metadata": {},
   "source": [
    "# Wybierz dwie obserwacje ze zbioru danych, które mają inne najważniejsze zmienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_shap_C = explainer.predict_parts(X_train.iloc[21,:], type='shap')\n",
    "pp_shap_C.plot()\n",
    "pp_shap_D = explainer.predict_parts(X_train.iloc[1,:], type='shap')\n",
    "pp_shap_D.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-player",
   "metadata": {},
   "source": [
    "Pierwszy:\n",
    "- `Total Revolving Bal` -0.141\n",
    "- `Moths Inactive 12 mon` 0.061\n",
    "\n",
    "\n",
    "Drugi:\n",
    "- `Total Relationship Count`-0.12\n",
    "- `Total Trans Amt` 0.069\n",
    "\n",
    "Dla pierwszej obserwacji zmienna `Total_Revolving_Bal` ma dużo większy udział niż dla drugiej. Klienci posiadający saldo zerowe nie mogą korzystać z usługi, co może być dużą motywacją do rezygnacji z konta. Wsród zmienych mający często duży udział w predykcji, największym udział dla konkretnej obserwacji mają zmienne, które przyjmują wartości brzegowe/ odchylone od średniej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_shap = explainer.predict_parts(X_train.iloc[26,:], type='shap')\n",
    "pp_shap.plot()\n",
    "pp_shap = explainer.predict_parts(X_train.iloc[30,:], type='shap')\n",
    "pp_shap.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-chance",
   "metadata": {},
   "source": [
    "Tak jak wcześniej zauważyłam, zmienne ktróe przyjmują swoje wartości brzegowe jak np. `Total_Relationship_Count`, mają dużą kontrybucję. Dodatkowo dla tej zmiennej można by przypuszczać że, to czy kontrybucja jest dodatnia lub ujemna zależy od tego którą wartość brzegową przyjmiemy - 1 lub 5. Podobnie wygląda to dla zmiennej `Total_Trans_Amt`. Przyjrzyjmy się także zmiennej `Months_Inactive_12_mon`. W obu przypadkach przyjmuję ona wartość 3, a jej kontrybucja wynosi -0.018 w pierwszym i -0.2 w drugi.Są to wartości podobne, ale nie identyczne."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
